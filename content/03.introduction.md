## Introduction

Biological research has become increasingly computational.
The field of genomics, in particular, has experienced a deluge of high-throughput sequencing data that has already revolutionized our understanding of the diversity and function of organisms and communities, building basic understanding from ecosystems to human health.
The biological analyses used to produce these insights often span hundreds of steps and involve a myriad of decisions ranging from small-scale tool and parameter choices to larger-scale decisions around computational experimental design and statistical analyses.
Each step relies not only upon code written by the user, but on software, its dependencies, and the compute infrastructure and operating system on which the code is executed.
Historically, this has led to patchwork availability of underlying code for analyses and a lack of interoperability of software and analysis pipelines across compute systems [@doi:10.1038/s41587-020-0439-x].
Combined with unmet training needs in biological data analysis, these conditions undermine the reuse of data and the reproducibility of biological research, vastly limiting the value of our generated data [@doi:10.1371/journal.pcbi.1005755].

The biological research community is strongly committed to addressing these issues, recently formalizing the idea that all life sciences research (including data and analysis workflows) should be Findable, Accessible, Interoperable, and Reusable (FAIR) [@doi:10.1016/j.cels.2018.03.014].
For computational analyses, these ideals are certainly achievable given current technologies, but implementing them in practice has proven quite difficult, particularly for biologists with insufficient training in computing [@doi:10.1016/j.cels.2018.03.014, @doi:10.1139/facets-2019-0020].
However, the recent maturation of data-centric workflow systems designed to automate and facilitate computational workflows are revolutionizing our capacity to conduct end-to-end FAIR analyses [@doi:10.1016/j.future.2017.05.041, @doi:10.1038/s41587-020-0439-x].
These workflow systems are designed to handle some aspects of computational workflows internally, namely, the interactions with software and computing infrastructure, and the ordered execution of each step.
By reducing the manual input and monitoring required at each analysis juncture, automated systems ensure that analyses are repeatable and enable them to be run at much larger scales.
For these reasons, workflow systems are rapidly becoming the workhorses of modern bioinformatics.

As the biological computing community has grown over the past decade, several papers have presented "best" or "good enough" practices that have been critical in steering biologists towards strategies that facilitate their research while also promoting reproducibility [@doi:10.1371/journal.pbio.1001745; @doi:10.1371/journal.pbio.1002303; @doi:10.1371/journal.pcbi.1005510].
Given the power of workflow systems for data-intensive biology



building well-documented and reusable workflows for their own research

Since the latest paper in 2017 [@doi:10.1371/journal.pcbi.1005510], the maturation of workflow and software management systems have greatly reduced the barriers to end-to-end reproducibility for biological data analyses.

Given the power of workflow systems to reduce the barrier to entry for reproducible computational analyses,  




With proper implementation, workflow management systems enable fully-contained workflows to be automated, scaleable, reproducible and reusable. ...robust to software updates, and executable across platforms.



Adopting workflow systems requires an up-front investment: for each step of an analysis, researchers must provide information concerning the required input and expected output data, the computing resources required, and any additional software required to run the step (often called a "rule").
This information can be used to determine the order in which each step should be executed, enabling the user to transfer responsibility for conditional execution of each step upon completion of its dependencies over to an automated system.
In return, the benefits provided are many-fold: these systems can keep track of large, branching workflows, install software as necessary, re-execute analysis steps upon failure (if expected outputs are not produced) or updates of upstream steps, distribute analyses across available resources on high-performance computing systems, and monitor resource use and progress, including optionally alerting the user upon completion.
The standardized information and syntax required for rule-based workflow specification means code is inherently modular and more easily transferable between projects [@doi:10.1007/s00778-005-0153-9; @doi:10.1016/j.future.2017.05.041].
This means the upfront cost of specifying a workflow in a system is mitigated as this code is reused, leading to faster time-to-insight [@doi:10.1016/j.future.2017.05.041; @doi:10.1007/s41019-017-0050-4].









Workflow systems simplify computational and software management tasks,


At the same time ..fair..



In concert, sharing of data analysis lessons and workflows has created a critical mass of analysis code now openly available for research and training, including that done by nonprofit organizations such as the Carpentries [@doi:10.2218/ijdc.v10i1.351].
This code can be easily reused and modified to accommodate advances in bioinformatics across diverse workflow system applications, further improving efficiency [@doi:10.1007/s41019-017-0050-4].







Given the computational tools that now exist, Gr√ºning et al ([@doi:10.1016/j.cels.2018.03.014]) noted in 2018 that not performing data analyses reproducibly is becoming "unjustifiable and inexcusable".
"we are reaching the point where not performing data analyses reproducibly becomes unjustifiable and inexcusable"
BUT
Cereceda and Quinn discuss the challenges of learning and interacting with OSS from a graduate student perspective, many of which are broadly applicable to biological researchers at any stage [@doi:10.1139/facets-2019-0020].
here - we try to address the need from training.
In our experiences with both research and training, workflow systems have greatly reduced the barrier to entry for data analysis at scale and opened the door to end-to-end reproducibility.
Here, we present a series "good enough" practices and tips for leveraging workflow systems to streamline data-intensive biology and to enhance the documentation, automation and reproducibility of your science.
