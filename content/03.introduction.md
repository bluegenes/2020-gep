## Introduction

Biological research has become increasingly computational.
The field of genomics, in particular, has experienced a deluge of high-throughput sequencing data that has already revolutionized our understanding of the diversity and function of organisms and communities, building basic understanding from ecosystems to human health.
The biological analyses used to produce these insights often span hundreds of steps and involve a myriad of decisions ranging from small-scale tool and parameter choices to larger-scale decisions around computational experimental design and statistical analyses.
Each step relies not only upon code written by the user, but on software, its dependencies, and the compute infrastructure and operating system on which the code is executed.
Historically, this has led to patchwork availability of underlying code for analyses and a lack of interoperability of software and analysis pipelines across compute systems [@doi:10.1038/s41587-020-0439-x].
Combined with unmet training needs in biological data analysis, these conditions undermine the reuse of data and the reproducibility of biological research, vastly limiting the value of our generated data [@doi:10.1371/journal.pcbi.1005755].

To address these issues, a rich tapestry of data-centric workflow systems emerged that were designed handle some aspects of computational analysis -- namely, the interactions with software and computing infrastructure -- under the surface, while enabling users to focus on writing and running code for biological analyses [@doi:10.1016/j.future.2017.05.041, @doi:10.1038/s41587-020-0439-x].
Initial tools struggled with limited functionality, incomplete software management solutions, complex coding requirements, and were often tailored to the needs of specific communities, hindering widespread adoption and utility.
However, recent maturation of both workflow and software tools has revolutionized data-intensive biology.
Workflow systems have quickly become the workhorses of modern bioinformatics, empowering researchers to execute dozens of analysis tools in a systematic manner across all experimental samples.

Current workflow systems separate analysis code written by the user from software and compute infrastructure, but manage and integrate both pieces [@doi:10.1038/s41587-020-0439-x].
This approach improves interoperability and therefore repeatability of workflows encoded in workflow systems.
Further, convergence on rule-based workflow specification means code is inherently modular and easily transferable between projects [@doi:10.1007/s00778-005-0153-9; @doi:10.1016/j.future.2017.05.041].
This means the upfront cost of specifying a workflow in a system is mitigated as this code is reused, leading to faster time-to-insight [@doi:10.1016/j.future.2017.05.041; @doi:10.1007/s41019-017-0050-4].
In concert, sharing of data analysis lessons and workflows has created a critical mass of analysis code now openly available for research and training, including that done by nonprofit organizations such as the Carpentries [@doi:10.2218/ijdc.v10i1.351].
This code can be easily reused and modified to accomodate advances in bioinformatics across diverse workflow system applications, further improving efficiency [@doi:10.1007/s41019-017-0050-4].
Taken together, modern workflow management systems enable fully-contained workflows to be automated, scaleable, robust to software updates, and executable across platforms.

Reproducibility is critical in biological research... here's how workflows help!

In 2016, a community of researchers, publishers, and funding agencies addressed this issue with a series of principles, "FAIR" (findable, accessible, interoperable, reusable) meant to guide biological data analysis [@doi:10.1016/j.cels.2018.03.014].

The "FAIR" (findable, accessible, interoperable, reusable) principles formalize the idea that published data -- and the workflows used to analyze them -- should be at minimum, written to be reproducible and reusable, and published so as to be permanently available to the research community [@doi:10.1016/j.cels.2018.03.014]..
While most researchers agree on the benefits that such practices provide, implementing them in practice is difficult, particularly for biologists with insufficient training in computing.
As the biological computing community has grown over the past decade, several papers have presented "best" or "good enough" practices for reproducible biological data analyses [@doi:10.1371/journal.pbio.1001745; @doi:10.1371/journal.pbio.1002303; @doi:10.1371/journal.pcbi.1005510].
The practices and recommendations provided therein have been critical in steering biologists new to computational analyses towards generating well-documented and reusable workflows.


Since the latest paper in 2017 [@doi:10.1371/journal.pcbi.1005510], the maturation of workflow and software management systems have greatly reduced the barriers to end-to-end reproducibility for biological data analyses.
Given the computational tools that now exist, Gr√ºning et al ([@doi:10.1016/j.cels.2018.03.014]) noted in 2018 that not performing data analyses reproducibly is becoming "unjustifiable and inexcusable".
"we are reaching the point where not performing data analyses reproducibly becomes unjustifiable and inexcusable"
BUT
Cereceda and Quinn discuss the challenges of learning and interacting with OSS from a graduate student perspective, many of which are broadly applicable to biological researchers at any stage [@doi:10.1139/facets-2019-0020].
here - we try to address the need from training.
In our experiences with both research and training, workflow systems have greatly reduced the barrier to entry for data analysis at scale and opened the door to end-to-end reproducibility.
Here, we present a series "good enough" practices and tips for leveraging workflow systems to streamline data-intensive biology and to enhance the documentation, automation and reproducibility of your science.
