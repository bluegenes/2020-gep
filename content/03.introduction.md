## Introduction

Biological research has become increasingly computational.
The field of genomics, in particular, has experienced a deluge of high-throughput sequencing data that has already revolutionized our understanding of the diversity and function of organisms and communities, building basic understanding from ecosystems to human health.
The analysis workflows used to produce these insights often span hundreds of steps and involve a myriad of decisions ranging from small-scale tool and parameter choices to larger-scale decisions around computational experimental design and statistical analyses.
Each step relies not only upon code written by the user, but on software, its dependencies, and the compute infrastructure and operating system on which the code is executed.
Historically, this has led to patchwork availability of underlying code for analyses and a lack of interoperability of software and analysis pipelines across compute systems [@doi:10.1038/s41587-020-0439-x].
Combined with unmet training needs in biological data analysis, these conditions undermine the reuse of data and the reproducibility of biological research, vastly limiting the value of our generated data [@doi:10.1371/journal.pcbi.1005755].

The biological research community is strongly committed to addressing these issues, recently formalizing the idea that all life sciences research (including data and analysis workflows) should be Findable, Accessible, Interoperable, and Reusable (FAIR) [@doi:10.1016/j.cels.2018.03.014].
For computational analyses, these ideals are achievable given current technologies, but implementing them in practice has proven difficult, particularly for biologists with insufficient training in computing [@doi:10.1016/j.cels.2018.03.014, @doi:10.1139/facets-2019-0020].
However, the recent maturation of data-centric workflow systems designed to automate and facilitate computational workflows are revolutionizing our capacity to conduct end-to-end FAIR analyses [@doi:10.1016/j.future.2017.05.041, @doi:10.1038/s41587-020-0439-x].
These workflow systems are designed to handle some aspects of computational workflows internally: namely, the interactions with software and computing infrastructure, and the ordered execution of each step.
By reducing the manual input and monitoring required at each analysis juncture, these integrated systems ensure that analyses are repeatable and can be executed at much larger scales.
In concert, the standardized information and syntax required for rule-based workflow specification makes code inherently modular and more easily transferable between projects [@doi:10.1007/s00778-005-0153-9; @doi:10.1016/j.future.2017.05.041].
For these reasons, workflow systems are rapidly becoming the workhorses of modern bioinformatics.

Adopting workflow systems requires some level of up-front investment, first to understand the structure of the system, and then to learn the workflow-specific syntax.
These challenges can preclude adoption, particularly for researchers without significant computational experience [@doi:10.1139/facets-2019-0020].
In our experiences with both research and training, these initial learning costs are similar to those required for learning more traditional analysis strategies, but then provide a myriad of additional benefits that both facilitate and accelerate research.
Furthermore, online communities for sharing reusable workflow code have proliferated, meaning the initial cost of specifying a workflow in a system is mitigated via use and re-use of common steps, leading to faster time-to-insight [@doi:10.1016/j.future.2017.05.041; @doi:10.1007/s41019-017-0050-4].

Building upon the rich literature of "best" and "good enough" practices for computational biology [@doi:10.1371/journal.pbio.1001745; @doi:10.1371/journal.pbio.1002303; @doi:10.1371/journal.pcbi.1005510], we present a series of strategies and "good enough" practices for adopting workflow systems to streamline data-intensive biology research. This manuscript is designed to help guide biologists, particularly those with insufficient computational training, towards project, data, and resource management strategies that facilitate and expedite reproducible data analysis in their research.
We present these strategies in the context of our own experiences working with high-throughput sequencing data, but many are broadly applicable to biologists working beyond this field.
