## Workflows facilitate data-intensive biology

Data-intensive biology typically requires that researchers execute computational workflows using multiple analytic tools and apply them to many experimental samples in a systematic manner.
These workflows commonly produce hundreds to thousands of intermediate files and require incremental changes as experimental insights demand tool and parameter modifications.
Many intermediate steps are central to the biological analysis, but others, such as converting between file formats, are rote computational tasks required to passage data from one tool to the next.
Some of these steps can fail silently, producing incomplete intermediate files that imperceptively invalidate downstream results and biological inferences.
Properly managing and executing all of these steps is vital, but can be both time-consuming and error-prone, even when automated with scripting languages such as bash.

The emergence and maturation of workflow systems designed with bioinformatic challenges in mind has revolutionized computing in data intensive biology [@doi:10.1038/s41592-018-0046-7].
Workflow systems contain powerful infrastructure for workflow management that can coordinate runtime behavior, self-monitor progress and resource usage, and compile reports documenting the results of a workflow (**Figure @fig:workflow**).
These features ensure that the steps for data analysis are repeatable and at least minimally described from start to finish.
When paired with proper software management, fully-contained workflows are scalable, robust to software updates, and executable across platforms, meaning they will likely still execute the same set of commands with little investment by the user after weeks, months, or years.

![**Workflow Systems:** Bioinformatic workflow systems have built-in functionality that facilitates and simplifies running analysis pipelines.
**A. Samples:** Workflow systems enable you to use the same code to run each step on each sample. Samples can be easily added if the analysis expands.
**B. Software Management:** Integration with software management tools (e.g. conda, singularity, docker) can automate software installation for each step.
**C. Branching, D. Parallelization, and E. Ordering:** Workflow systems handle conditional execution, ensuring that tasks are executed in the correct order for each sample file, including executing independent steps in parallel if possible given the resources provided.
**F. Standard Steps:** Many steps are now considered "standard" (e.g. quality control). Workflow languages keep all information for a step together and can be written to enable you to remix and reuse individual steps across pipelines.
**G. Rerun as necessary:** Workflow systems keep track of which steps executed properly and on which samples, and allow you to rerun failed steps (or additional steps) rather than re-executing the entire workflow.
**H. Reporting:** Workflow languages enable comprehensive reporting on workflow execution and resource utilization by each tool.
**I. Portability:** Analyses written in workflow languages (with integrated software management) can be run across computing systems without changes to code.](images/workflow_figure.svg){#fig:workflow}

To properly direct an analysis, workflow systems need to encode information about the relationships between every workflow step.
In practice, this means that each analysis step must specify the input (or types of inputs) needed for that step, and the output (or types of outputs) being produced.
This structure provides several additional benefits.
First, workflows become minimally self-documented, as the directed graph produced by workflow systems can be exported and visualized, producing a graphical representation of the relationships between all steps in a pipeline (see **Figure @fig:rnaseq_workflow**).
Next, workflows are more likely to be fully enclosed without undocumented steps that are executed by hand, meaning analyses are more likely to be reproducible.
Finally, each step becomes a self-contained unit that can be used and re-used across multiple analysis workflows, so scientists can spend less time implementing standard steps, and more time on their specific research questions.
In sum, the internal scaffolding provided by workflow systems helps build analyses that are generally better documented, repeatable, transferable, and scalable.


#### Getting started with workflows

The workflow system you choose will be largely dependent on your analysis needs.
Here, we draw a distinction between two types of workflows: "research" workflows that are under iterative development to answer novel scientific questions, and "production" workflows, which have reached maturity and are primarily used to run a standard analysis on new samples.
In particular, research workflows require flexibility and assessment at every step: outliers and edge cases may reveal interesting biological differences, rather than sample processing or technical errors.
Many workflow systems can be used for either type, but we note cases where their properties facilitate one of these types over the other.

**Using workflows without learning workflow syntax**
While the benefits of executing an analysis within a data-centric workflow system are immense, the learning curve associated with command-line systems can be daunting.
It is possible to obtain the benefits of workflow systems without learning new syntax.
Websites like Galaxy, Cavatica, and EMBL-EBI MGnify offer online portals in which users build workflows around publicly-available or user-uploaded data [@doi:10.1093/nar/gky379; @doi:10.14694/EDBK_175029; @doi:10.1093/nar/gkz1035].
On the command line, many research groups have used workflow systems to wrap one or many analysis steps (specified in an underlying workflow language) in a more user-friendly command-line application that accepts user input and executes the analysis.
These pipeline applications allow users to take advantage of the benefits of workflow software without needing to write the workflow syntax for each analysis step.
Some examples include the nf-core RNA-seq pipeline [@url:https://github.com/nf-core/rnaseq/; @doi:10.1038/s41587-020-0439-x], the PiGx genomic analysis toolkit [@doi:10.1093/gigascience/giy123], the ATLAS metagenome assembly and binning pipeline [@url:https://github.com/metagenome-atlas/atlas; @doi:10.1101/737528], the Sunbeam metagenome analysis pipeline [@url:https://github.com/sunbeam-labs/sunbeam; @doi:10.1186/s40168-019-0658-x], and two from our own lab, the dammit eukaryotic transcriptome annotation pipeline [@url:https://github.com/dib-lab/dammit] and the elvers *de novo* transcriptome pipeline [@url:https://github.com/dib-lab/elvers].
These pipeline applications typically execute a series of standard steps, but many provide varying degrees of customizability ranging from tool choice to parameter specification.

**Choosing a workflow system**
If your use case extends beyond these tools, there are several scriptable workflow systems that offer comparable benefits for carrying out your own data-intensive analyses.
Each has it own strengths, meaning each workflow software will meet an individuals computing goals differently (see **Table @tbl:workflows**).
Our lab has adopted Snakemake [@doi:10.1093/bioinformatics/bts480 @doi:10.5281/zenodo.4067137], in part due to its similarity and integration with Python, its flexibility for building and testing new analyses in different languages, and its intuitive integration with software management tools (described below).
Snakemake and Nextflow [@doi:10.1038/nbt.3820] are commonly used for developing new research pipelines, where flexibility and iterative, branching development is a key feature. Common Workflow Language (CWL) and Workflow Description Language (WDL) are workflow specification formats that are more geared towards scalability, making them ideal for production-level pipelines with hundreds of thousands of samples [@doi:10.6084/m9.figshare.3115156.v2]. WDL and CWL are commonly executed on platforms such as Terra [@url:https://terra.bio] or Seven Bridges Platform [@url:https://www.sevenbridges.com/platform/].
Language-specific workflow systems, such as ROpenSci's Drake [@doi:10.21105/joss.00550], are more limited in the scope of tasks they can execute, but are powerful within their language and easier to integrate for those comfortable with that language.


|Workflow System | Documentation | Example Workflow | Tutorial |
|----------------|---------------|------------------|------------------|
| Snakemake | https://snakemake.readthedocs.io/ | https://github.com/snakemake-workflows/chipseq | https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html |
| Nextflow | https://www.nextflow.io/ | https://github.com/nf-core/sarek | https://www.nextflow.io/docs/latest/getstarted.html |
| Common workflow language | https://www.commonwl.org/ | https://github.com/EBI-Metagenomics/pipeline-v5 | https://www.commonwl.org/user_guide/02-1st-example/index.html |
| Workflow description language | https://openwdl.org/ | https://github.com/gatk-workflows/gatk4-data-processing |  https://support.terra.bio/hc/en-us/articles/360037127992--1-howto-Write-your-first-WDL-script-running-GATK-HaplotypeCaller |
Table: Four of the most widely used bioinformatics workflow systems (2020), with links to documentation, example workflows, and general tutorials.
In many cases, there may be tutorials online that are tailored for use cases in your field.
All of these systems can interact with tools or tasks written in other languages and can function across cloud computing systems and high-performance computing clusters.
Some can also import full workflows from other specification languages.
{#tbl:workflows}


The best workflow system to choose may be the one with a strong and accessible local or online community in your field, somewhat independent of your computational needs.
The availability of field-specific data analysis code for reuse and modification can facilitate the adoption process, as can community support for new users.
Fortunately, the standardized syntax required by workflow systems, combined with widespread adoption in the open science community, has resulted in a proliferation of open access workflow-system code for routine analysis steps [@doi:10.1007/978-1-4939-9074-0_24, @doi:10.1038/s41587-020-0439-x; @doi:10.21105/joss.00352].
At the same time, consensus approaches for data analysis are emerging, further encouraging reuse of existing code [@doi:10.1186/s13059-016-0881-8; @doi:10.1038/nbt.3935; @doi:10.15252/msb.20188746; @doi:10.1016/j.margen.2016.04.012; @doi:10.1038/s41579-018-0029-9].

The [Getting started developing workflows](### getting-started-developing-workflows) section contains strategies for modifying and developing workflows for your own analyses.

### Wrangling Scientific Software

Analysis workflows commonly rely on multiple software packages to generate final results.
These tools are heterogeneous in nature: they are written by researchers working in different coding languages, with varied approaches to software design and optimization, and often for specific analysis goals.
Each program has a number of other programs it depends upon to function ("dependencies"), and as software changes over time to meet research needs, the results may change, even when run with identical parameters.
As a result, it is critical to take an organized approach to installing, managing, and keeping track of software and software versions.
On many compute systems, system-wide software management is overseen by system administrators, who ensure commonly-used and requested software is installed into a "module" system available to all users.
Unfortunately, this system limits software version transparency and does not lend itself well to exploring new workflows and software, as researchers do not have permission to install software themselves.
To meet this need, most workflow managers integrate with software management systems that handle software installation, management, and packaging, alleviating problems that arise from complex dependencies and facilitating documentation of software versions.
Software management systems range from lightweight systems that manage only the software and its dependencies, to heavyweight systems that control for all aspects of the runtime and operating system, ensuring 100% reproducibility of results across computational platforms and time.

On the lightweight end, the conda package manager has emerged as a leading software management solution for research workflows (**Figure @fig:conda_figure_ABC**).
Conda handles both cluster permission and version conflict issues with a user-based software environment system, and features a straightforward "recipe" system which simplifies the process of making new software installable (including simple management of versions and updates).
These features have led to widespread adoption within the bioinformatic community: packages for new software become quickly available, and can be installed easily across platforms.
However, conda does not attempt to isolate software installations or standardize the runtime environment, meaning installations will not be completely reproducible over time.
Heavyweight software management systems package not only the software of interest, but also the runtime environment information, with the goal of ensuring perfect reproducibility in software installation over time.
Tools such as singularity and docker [@doi:10.1016/j.cels.2018.03.014; @doi:10.1038/s41592-018-0046-7; @doi:10.1371/journal.pone.0177459; @url:https://dl.acm.org/doi/10.5555/2600239.2600241] wrap software environments in "containers" that capture and reproduce the runtime environment information.
Container-based management is particularly useful for systems where some dependencies may not be installable by lightweight managers. However, software installation within these containers can be limited by similar reproducibility issues, including changes in dependency installations over time.
"Functional package managers" such as GNU Guix and Nix strictly require all dependency and configuration details be encoded within each software package, providing the most comprehensively reproducible installations.
These have begun to be integrated into some bioinformatic tools, but have a steeper learning curve for independent use [@doi:10.1093/gigascience/giy123].


![**The conda package and environment manager simplifies software installation and management.**
**A. Conda Recipe Repositories:** Each program distributed via Conda has a "recipe" describing all software dependencies needed for installation using Conda (each of which must also be installable via Conda). Recipes are stored and managed in the cloud in separate "channels", some of which specialize in particular fields or languages (e.g. the "bioconda" channel specializes in bioinformatic software, the "r" channel specializes in R language packages) [@doi:10.1038/s41592-018-0046-7]. **B. Use Conda Environments to Avoid Installation Conflicts:**  Conda does not require root
privileges for software installation, thus enabling use by researchers working on shared cluster systems. However, even user-based software installation can encounter dependency conflicts. For example, you might need to use python2 to install and run a program (e.g. older scripts written by members of your lab), while also using snakemake to execute your workflows (requires python>=3.5). By installing each program into an isolated "environment" that contains only the software required to run that
program, you can ensure all programs used throughout your analysis will run without issue. Using small, separate environments for your software and building many simple environments to accommodate different steps in your workflow is critical for reducing the amount of time it takes conda to resolve dependency conflicts between different software tools ("solve" an environment). Conda virtual environments can be created and installed either on the command line, or via an environment YAML file, as shown. In this case, the environment file
also specifies which conda channels to search and download programs from. When specified in a YAML file, conda environments are easily transferable between computers and operating systems. Further, because the version of each package installed in an environment is recorded, workflow reproducibility is enhanced. While Conda may be superseded by alternative solutions [@url:https://github.com/QuantStack/mamba], this model of software installation and management will likely
persist.](images/conda_figure_ABC.svg){#fig:conda_figure_ABC height=7in}


#### Getting started with software management

**Using software without learning software management systems**
First, there are a number of ways to test software before needing to worry about installation.
Some software packages are available as web-based tools and through a series of data upload and parameter specifications, allow the user to interact with a tool that is running on a back-end server.
Integrated development environments (IDE) like PyCharm and RStudio can manage software installation for language-specific tools, and can be very helpful when writing analysis code.
While these approaches do not integrate into reproducible workflows, they may be ideal for testing a tool to determine whether it is useful for your data before integration in your analysis.

**Choosing a software management system**
It is important to balance the time needed to learn to properly use a software management system with the needs of both the project and the researchers.
Software management systems with large learning curves are less likely to be widely adopted among researchers with a mix of biological and computational backgrounds.
In our experience, software management with conda nicely balances reproducibility with flexibility and ease of use.
These trade-offs are best for research workflows under active development, where flexible software installation solutions that enable new analysis explorations or regular tool updates are critical.
For production workflows that require maximal reproducibility, it is worth the larger investment required to use heavyweight systems.
This is particularly true for advanced users who can more easily navigate the steps required for utilizing these tools.
Container-based software installation via docker and singularity are common for production-level workflows, and Guix and Nix-based solutions are gaining traction.
Importantly, the needs and constraints of a project can evolve over time, as may the system of choice.


**Integrating software management within workflows**
Workflow systems provide seamless integration with a number of software management tools.
Each workflow system requires different specification for initiation of software management, but typically requires about one additional line of code per step that requires the use of software.
If the software management tool is installed locally, the workflow will automatically download and install the specified environment or container and use it for specified step.

In our experience, the complete solution for using scientific software involves a combination of approaches. Interactive and exploratory analyses conducted in IDEs and jupyter notebooks (usually with local software installation with conda) are useful for developing an analysis strategy and creating an initial workflow. This is then followed by workflow-integrated software management via conda, singularity, or nixOS for executing the resulting workflow on many samples. This process is iterative as we build and extend our analyses.
