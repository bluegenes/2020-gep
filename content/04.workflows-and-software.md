## Workflows facilitate data-intensive biology

Data-intensive biology typically requires that researchers execute computational workflows using multiple analytic tools and apply them to many experimental samples in a systematic manner.
These workflows commonly produce hundreds to thousands of intermediate files and require incremental changes as experimental insights demand tool and parameter modifications.
Many steps are central to the biological analysis, but others, such as converting between file formats, are rote computational tasks required to passage data from one tool to the next.
Properly managing and executing all of these steps is vital, but can be time-consuming and error-prone, even when automated using scripting languages (e.g. bash).

The emergence and maturation of workflow systems designed with bioinformatic challenges in mind has revolutionized computing in data intensive biology [@doi:10.1038/s41592-018-0046-7].
Workflow systems contain powerful infrastructure for workflow management that can coordinate runtime behavior, self-monitor progress and resource usage, and compile reports documenting the results of a workflow (**Figure @fig:workflow**).
These features ensure that the steps for data analysis are documented and repeatable from start to finish.
When paired with proper software management, fully-contained workflows are scalable, robust to software updates, and executable across platforms, meaning they will likely still execute the same set of commands with little investment by the user after weeks, months, or years.

![**Workflow Systems:** Bioinformatic workflow systems have built-in functionality that facilitates and simplifies running analysis pipelines.
**A. Samples:** Workflow systems enable you to use the same code to run each step on each sample. Samples can be easily added if the analysis expands.
**B. Software Management:** Integration with software management tools (e.g. conda, singularity, docker) can automate software installation for each step.
**C. Branching, F. Ordering, G. Parallelization:** Workflow systems handle conditional execution, ensuring that tasks are executed in the correct order for each sample file, including executing independent steps in parallel if possible given the resources provided.
**D. Standard Steps:** Many steps are now considered "standard" (e.g. quality control). Workflow languages keep all information for a step together and can be written to enable you to remix and reuse individual steps across pipelines.
**E. Rerun as necessary:** Workflow systems keep track of which steps executed properly and on which samples, and allow you to rerun failed steps (or additional steps) rather than re-executing the entire workflow.
**H. Reporting:** Workflow languages enable comprehensive reporting on workflow execution and resource utilization by each tool.
**I. Portability:** Analyses written in workflow languages (with integrated software management) can be run across computing systems without changes to code.](images/workflow_figure.svg){#fig:workflow}

To properly direct a workflow, workflow systems need to encode information about the relationships between workflow steps.
In practice, this means that each analysis step must specify the input (or types of inputs) needed for that step, and the output (or types of outputs) being produced.
This structure provides several additional benefits.
First, workflows become minimally self-documented, as the directed graph produced by workflow systems can be exported and visualized, producing a graphical representation of the relationships between all steps in a pipeline (see **Figure @fig:sgc_workflow**).
Next, workflows are more likely to be fully enclosed without undocumented steps that are executed by hand, meaning analyses are more likely to be reproducible.
Finally, each step becomes a self-contained unit that can be used and re-used across multiple analysis workflows, so scientists can spend less time implementing standard steps, and more time on their specific research questions.
In sum, the internal scaffolding provided by workflow systems helps build analyses that are generally better documented, repeatable, transferable, and scalable.


#### Getting started with workflows

The workflow system you choose will be largely dependent on your analysis needs.
Here, we draw a distinction between two types of workflows: "development" workflows that are under iterative development, and
"production" workflows, which have reached maturity and are mostly fixed. Many workflow systems can be used for either type, but we note cases where their properties facilitate one of these types over the other.

**Using workflows without learning workflow systems** The benefits of encoding a workflow in a workflow system are immense, but the learning curve associated with implementing complete workflows in a new syntax can be daunting.
It is possible to obtain the benefits of workflow systems without learning a workflow system.
Websites like Galaxy, Cavatica, and EMBL-EBI MGnify offer online portals in which users build workflows around publicly-available or user-uploaded data [@doi:10.1093/nar/gky379; @doi:10.14694/EDBK_175029; @doi:10.1093/nar/gkz1035].
On the command line, many research groups have used workflow systems to build user-friendly pipelines that do not require learning or working with the underlying workflow software.
These tools are specified in an underlying workflow language, but are packaged in a user-friendly command-line script that coordinates and executes the workflow.
Rather than writing each workflow step, the user can specify data and parameters in a configuration file to customize the run.
Some examples include the nf-core RNA-seq pipeline [@url:https://github.com/nf-core/rnaseq/; @doi:10.1038/s41587-020-0439-x], the ATLAS metagenome assembly and binning pipeline [@url:https://github.com/metagenome-atlas/atlas; @doi:10.1101/737528], the Sunbeam metagenome analysis pipeline [@url:https://github.com/sunbeam-labs/sunbeam; @doi:10.1186/s40168-019-0658-x], and two from our own lab, the dammit eukaryotic transcriptome annotation pipeline [@url:https://github.com/dib-lab/dammit] and the elvers *de novo* transcriptome pipeline [@url:https://github.com/dib-lab/elvers].
These tools allow users to take advantage of the benefits of workflow software without needing to invest in curating and writing their own pipeline. The majority of these workflows are production-level workflows designed to execute a series of standard steps, but many provide varying degrees of customizability ranging from tool choice to parameter specification.

**Choosing a workflow system**
If your use case extends beyond these tools, there are several scriptable workflow systems that offer comparable benefits for data intensive biology.
Each has it own strengths, meaning each software will meet an individuals computing goals differently (see **Table @tbl:workflows**).
Our lab has adopted Snakemake, in part due to its similarity and integration with Python, its flexibility for building and testing new analyses in different languages, and its intuitive integration with software management tools (described below)[@doi:10.1093/bioinformatics/bts480].
Snakemake and Nextflow are commonly used for designing new research pipelines, where flexibility and iterative, branching development is a key feature [@doi:10.1038/nbt.3820]. Common Workflow Language (CWL) and Workflow Description Language (WDL) are interchangeable formats that are more geared towards scalability, making them ideal for production-level pipelines with hundreds of thousands of samples [@doi:10.6084/m9.figshare.3115156.v2]. CWL and WDL are currently best built and used through wrapper tools such as Rabix or platforms such as Terra [@https://rabix.io; @https://terra.bio].
Language-specific workflow systems, such as ROpenSci's Drake [@doi:10.21105/joss.00550], are limited in the scope of tasks they can execute, but are powerful within their language and easier to integrate for those comfortable with that language.


|Workflow System | Documentation | Example Workflow | Tutorial |
|----------------|---------------|------------------|------------------|
| Snakemake | https://snakemake.readthedocs.io/ | https://github.com/snakemake-workflows/chipseq | https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html |
| Nextflow | https://www.nextflow.io/ | https://github.com/nf-core/sarek | https://www.nextflow.io/docs/latest/getstarted.html |
| Common workflow language | https://www.commonwl.org/ | https://github.com/EBI-Metagenomics/pipeline-v5 | https://www.commonwl.org/user_guide/02-1st-example/index.html |
| Workflow description language | https://openwdl.org/ | https://github.com/gatk-workflows/gatk4-data-processing |  https://support.terra.bio/hc/en-us/articles/360037127992--1-howto-Write-your-first-WDL-script-running-GATK-HaplotypeCaller |
Table: Popular bioinformatics workflow systems, documentation, example workflows, and tutorials.
While we have linked to general tutorials, there may be more relevant tutorials online for your field.
Some workflow systems are not programming language-specific: they can interact with tools or tasks written in other languages, and some can even import workflows from other specification languages.
{#tbl:workflows}


The best workflow system to choose may be the one with a strong and accessible local or online community in your field, somewhat independent of your computational needs.
The availability of field-specific data analysis code for reuse and modification can facilitate the adoption process, as can community support for new users.
Fortunately, the standardized syntax required by workflow systems, combined with widespread adoption in the open science community, has resulted in a proliferation of open access workflow-system code for routine analysis steps [@doi:10.1007/978-1-4939-9074-0_24, @doi:10.1038/s41587-020-0439-x; @doi:10.21105/joss.00352].
At the same time, consensus approaches for data analysis are emerging, further encouraging reuse of existing code [@doi:10.1186/s13059-016-0881-8; @doi:10.1038/nbt.3935; @doi:10.15252/msb.20188746; @doi:10.1016/j.margen.2016.04.012; @doi:10.1038/s41579-018-0029-9].

The [Getting started developing workflows](### getting-started-developing-workflows) section contains strategies for modifying and developing workflows for your own analyses.

### Wrangling Scientific Software

Analysis workflows commonly rely on multiple software packages to generate final results.
These tools are heterogeneous in nature: they are written by researchers working in different coding languages, with varied approaches to software design and optimization, and often for specific analysis goals.
Each program has a number of other programs it depends upon to function ("dependencies"), and as software changes over time to meet research needs, the results may change, even when run with identical parameters.
As a result, it is critical to take an organized approach to installing, managing, and keeping track of software and software versions.
To meet this need, most workflow managers integrate with software management systems like conda, singularity, and docker [@doi:10.1038/s41592-018-0046-7; @doi:10.1371/journal.pone.0177459; @https://dl.acm.org/doi/10.5555/2600239.2600241]. (CTB: one of these citations fails.)

Software management systems perform some combination of software installation, management, and packaging that alleviate problems that arise from dependencies and that facilitate documentation of software versions.
On many systems, system-wide software management is overseen by system administrators, who ensure commonly-used and requested software is installed into a "module" system available to all users.
Unfortunately, this system does not lend itself well for exploring new workflows and software, as researchers do not have permission to install software themselves.
The Conda package manager has emerged as a leading solution, largely because it handles both cluster permission and version conflict issues with a user-based software environment system, and features a straightforward "recipe" system which simplifies the process of making new software installable (**Figure @fig:conda_figure**).
Conda enables lightweight software installation and can be used with the same commands across platforms, but can still be impacted by differences in the host operating system.
Alternatively, wrapping software environments in "containers" that capture and reproduce all other aspects of the runtime environment can enhance reproducibility over time [@doi:10.1016/j.cels.2018.03.014].
Container-based software installation via docker and singularity is common for production-level workflows.


![**The conda package and environment manager simplifies software installation and management.**
**A. Conda Recipe Repositories:** Each program distributed via Conda has a "recipe" describing all software dependencies needed for Conda installation (each of which must also be installable via conda). These are stored and managed in separate "channels", some of which specialize (e.g. "bioconda" specializes in bioinformatic software, "r" specializes in R language packages) [@doi:10.1038/s41592-018-0046-7]. **B. Use Conda Environments to Avoid Installation Conflicts:**  Conda does not require root
privileges for software installation, thus enabling use by researchers working on shared cluster systems. However, even user-based software installation can encounter dependency conflicts. For example, you might need to use python2 to install and run a program (e.g. older scripts written by members of your lab), while also using snakemake to execute your workflows (requires python>=3.5). By installing each program into an isolated "environment" that contains only the software required to run that
program, you can ensure all programs will run without issue. Using small, separate environments for your software and building many simple environments to accommodate different steps in your workflow also reduces the amount of time it takes conda to resolve dependency conflicts between different software tools ("solve" an environment). Conda virtual environments can be created and installed either on the command line, or via an environment YAML file, as shown. In this case, the environment file
also specifies which Conda channels to search and download programs from. When specified in a YAML file, conda environments are easily transferable between computers and operating systems. Further, because the version of each package installed in an environment is recorded, workflow reproducibility is enhanced. Although portions of conda may be superseded by alternative solutions [@url:https://github.com/QuantStack/mamba], this model of software installation and management will likely
persist.](images/conda_figure.svg){#fig:conda_figure height=8in}

#### Getting started with software management


*Using software without learning management systems*
While package managers and containers greatly increase reproducibility, there are a number of ways to test software before needing to worry about installation.
Some software packages are available as web-based tools and through a series of data upload and parameter specifications, allow the user to interact with a tool that is running on a back-end server.
Integrated development environments (IDE) like PyCharm and RStudio can manage software installation for language-specific tools, and can be very helpful when writing analysis code.
These approaches are ideal for testing a tool to determine whether it produces useful output on your data before integration with your reproducible workflow.

**Integrating software management within workflows**
Workflow systems provide seamless integration with software management tools.
Each workflow requires different specification for initiation of software management, but typically requires about one additional line of code per step using software.
If the software management tool is installed locally, the workflow will automatically download and install the specified environment or container and use it for specified step.

In our experience, the complete solution for using scientific software involves starting with a combination of interactive and exploratory analyses in IDEs and local conda installation to develop an analysis strategy and create an initial workflow.  This is then followed by workflow-integrated software management via conda, singularity, or docker for executing the resulting workflow on many samples.
